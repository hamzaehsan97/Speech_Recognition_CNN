Using TensorFlow backend.
wandb: WARNING Run.description is deprecated. Please use wandb.init(notes="long notes") instead.
Saving vectors of label - 'bed':   0%|          | 0/1713 [00:00<?, ?it/s]Saving vectors of label - 'bed':   1%|1         | 21/1713 [00:00<00:08, 204.42it/s]Saving vectors of label - 'bed':   3%|2         | 48/1713 [00:00<00:07, 218.49it/s]Saving vectors of label - 'bed':   4%|4         | 72/1713 [00:00<00:07, 223.45it/s]Saving vectors of label - 'bed':   6%|5         | 100/1713 [00:00<00:06, 236.20it/s]Saving vectors of label - 'bed':   7%|7         | 127/1713 [00:00<00:06, 244.95it/s]Saving vectors of label - 'bed':   9%|8         | 154/1713 [00:00<00:06, 249.37it/s]Saving vectors of label - 'bed':  10%|#         | 178/1713 [00:00<00:06, 245.92it/s]Saving vectors of label - 'bed':  12%|#1        | 204/1713 [00:00<00:06, 249.46it/s]Saving vectors of label - 'bed':  13%|#3        | 228/1713 [00:00<00:06, 237.96it/s]Saving vectors of label - 'bed':  15%|#4        | 252/1713 [00:01<00:06, 227.91it/s]Saving vectors of label - 'bed':  16%|#6        | 275/1713 [00:01<00:06, 222.11it/s]Saving vectors of label - 'bed':  18%|#7        | 301/1713 [00:01<00:06, 231.79it/s]Saving vectors of label - 'bed':  19%|#9        | 328/1713 [00:01<00:05, 241.61it/s]Saving vectors of label - 'bed':  21%|##        | 355/1713 [00:01<00:05, 246.93it/s]Saving vectors of label - 'bed':  22%|##2       | 380/1713 [00:01<00:05, 235.46it/s]Saving vectors of label - 'bed':  24%|##3       | 404/1713 [00:01<00:06, 215.36it/s]Saving vectors of label - 'bed':  25%|##4       | 426/1713 [00:01<00:06, 199.80it/s]Saving vectors of label - 'bed':  26%|##6       | 447/1713 [00:02<00:07, 168.36it/s]Saving vectors of label - 'bed':  27%|##7       | 466/1713 [00:02<00:08, 144.31it/s]Saving vectors of label - 'bed':  28%|##8       | 482/1713 [00:02<00:10, 119.39it/s]Saving vectors of label - 'bed':  29%|##8       | 496/1713 [00:02<00:10, 113.20it/s]Saving vectors of label - 'bed':  30%|##9       | 509/1713 [00:02<00:10, 115.05it/s]Saving vectors of label - 'bed':  31%|###       | 524/1713 [00:02<00:09, 122.27it/s]Saving vectors of label - 'bed':  32%|###1      | 542/1713 [00:02<00:08, 133.56it/s]Saving vectors of label - 'bed':  33%|###2      | 557/1713 [00:03<00:10, 113.49it/s]Saving vectors of label - 'bed':  33%|###3      | 570/1713 [00:03<00:10, 109.17it/s]Saving vectors of label - 'bed':  34%|###3      | 582/1713 [00:03<00:10, 111.36it/s]Saving vectors of label - 'bed':  35%|###4      | 594/1713 [00:03<00:10, 111.37it/s]Saving vectors of label - 'bed':  35%|###5      | 606/1713 [00:03<00:09, 112.94it/s]Saving vectors of label - 'bed':  36%|###6      | 618/1713 [00:03<00:10, 107.65it/s]Saving vectors of label - 'bed':  37%|###6      | 633/1713 [00:03<00:09, 116.87it/s]Saving vectors of label - 'bed':  38%|###7      | 646/1713 [00:03<00:08, 120.28it/s]Saving vectors of label - 'bed':  38%|###8      | 659/1713 [00:03<00:09, 115.28it/s]Saving vectors of label - 'bed':  39%|###9      | 674/1713 [00:04<00:08, 123.06it/s]Saving vectors of label - 'bed':  40%|####      | 687/1713 [00:04<00:08, 122.69it/s]Saving vectors of label - 'bed':  41%|####      | 702/1713 [00:04<00:07, 128.53it/s]Saving vectors of label - 'bed':  42%|####1     | 716/1713 [00:04<00:07, 127.90it/s]Saving vectors of label - 'bed':  43%|####2     | 731/1713 [00:04<00:07, 133.20it/s]Saving vectors of label - 'bed':  43%|####3     | 745/1713 [00:04<00:07, 130.38it/s]Saving vectors of label - 'bed':  44%|####4     | 760/1713 [00:04<00:07, 135.43it/s]Saving vectors of label - 'bed':  45%|####5     | 774/1713 [00:04<00:07, 132.99it/s]Saving vectors of label - 'bed':  46%|####6     | 795/1713 [00:04<00:06, 146.39it/s]Saving vectors of label - 'bed':  47%|####7     | 811/1713 [00:04<00:06, 139.37it/s]Saving vectors of label - 'bed':  49%|####8     | 831/1713 [00:05<00:05, 151.30it/s]Saving vectors of label - 'bed':  50%|####9     | 856/1713 [00:05<00:05, 170.34it/s]Saving vectors of label - 'bed':  51%|#####1    | 879/1713 [00:05<00:04, 184.40it/s]Saving vectors of label - 'bed':  52%|#####2    | 899/1713 [00:05<00:04, 181.27it/s]Saving vectors of label - 'bed':  54%|#####3    | 918/1713 [00:05<00:04, 173.38it/s]Saving vectors of label - 'bed':  55%|#####4    | 938/1713 [00:05<00:04, 179.77it/s]Saving vectors of label - 'bed':  56%|#####6    | 962/1713 [00:05<00:03, 193.58it/s]Saving vectors of label - 'bed':  58%|#####7    | 987/1713 [00:05<00:03, 206.76it/s]Saving vectors of label - 'bed':  59%|#####9    | 1012/1713 [00:05<00:03, 217.10it/s]Saving vectors of label - 'bed':  60%|######    | 1035/1713 [00:06<00:03, 208.38it/s]Saving vectors of label - 'bed':  62%|######1   | 1060/1713 [00:06<00:02, 218.93it/s]Saving vectors of label - 'bed':  63%|######3   | 1086/1713 [00:06<00:02, 227.58it/s]Saving vectors of label - 'bed':  65%|######4   | 1110/1713 [00:06<00:02, 214.04it/s]Saving vectors of label - 'bed':  66%|######6   | 1132/1713 [00:06<00:02, 205.14it/s]Saving vectors of label - 'bed':  67%|######7   | 1153/1713 [00:06<00:02, 187.70it/s]Saving vectors of label - 'bed':  68%|######8   | 1173/1713 [00:06<00:02, 186.03it/s]Saving vectors of label - 'bed':  70%|######9   | 1193/1713 [00:06<00:02, 188.56it/s]Saving vectors of label - 'bed':  71%|#######   | 1213/1713 [00:06<00:02, 173.11it/s]Saving vectors of label - 'bed':  72%|#######1  | 1231/1713 [00:07<00:02, 173.74it/s]Saving vectors of label - 'bed':  73%|#######2  | 1249/1713 [00:07<00:02, 166.93it/s]Saving vectors of label - 'bed':  74%|#######3  | 1267/1713 [00:07<00:02, 170.29it/s]Saving vectors of label - 'bed':  75%|#######5  | 1291/1713 [00:07<00:02, 185.79it/s]Saving vectors of label - 'bed':  77%|#######6  | 1316/1713 [00:07<00:01, 200.47it/s]Saving vectors of label - 'bed':  78%|#######8  | 1337/1713 [00:07<00:01, 201.65it/s]Saving vectors of label - 'bed':  79%|#######9  | 1358/1713 [00:07<00:01, 196.24it/s]Saving vectors of label - 'bed':  81%|########  | 1379/1713 [00:07<00:01, 182.61it/s]Saving vectors of label - 'bed':  82%|########1 | 1398/1713 [00:07<00:01, 172.36it/s]Saving vectors of label - 'bed':  83%|########2 | 1420/1713 [00:08<00:01, 182.63it/s]Saving vectors of label - 'bed':  84%|########4 | 1440/1713 [00:08<00:01, 186.09it/s]Saving vectors of label - 'bed':  85%|########5 | 1462/1713 [00:08<00:01, 193.70it/s]Saving vectors of label - 'bed':  87%|########6 | 1484/1713 [00:08<00:01, 200.51it/s]Saving vectors of label - 'bed':  88%|########7 | 1507/1713 [00:08<00:01, 205.32it/s]Saving vectors of label - 'bed':  89%|########9 | 1531/1713 [00:08<00:00, 212.52it/s]Saving vectors of label - 'bed':  91%|######### | 1553/1713 [00:08<00:00, 198.09it/s]Saving vectors of label - 'bed':  92%|#########2| 1576/1713 [00:08<00:00, 205.73it/s]Saving vectors of label - 'bed':  93%|#########3| 1598/1713 [00:08<00:00, 208.19it/s]Saving vectors of label - 'bed':  95%|#########4| 1620/1713 [00:09<00:00, 192.37it/s]Saving vectors of label - 'bed':  96%|#########5| 1640/1713 [00:09<00:00, 184.53it/s]Saving vectors of label - 'bed':  97%|#########6| 1659/1713 [00:09<00:00, 174.04it/s]Saving vectors of label - 'bed':  98%|#########7| 1677/1713 [00:09<00:00, 158.74it/s]Saving vectors of label - 'bed':  99%|#########8| 1694/1713 [00:09<00:00, 137.05it/s]Saving vectors of label - 'bed': 100%|#########9| 1709/1713 [00:09<00:00, 131.92it/s]Saving vectors of label - 'bed': 100%|##########| 1713/1713 [00:09<00:00, 175.10it/s]
Saving vectors of label - 'cat':   0%|          | 0/1733 [00:00<?, ?it/s]Saving vectors of label - 'cat':   1%|          | 17/1733 [00:00<00:10, 163.90it/s]Saving vectors of label - 'cat':   2%|2         | 41/1733 [00:00<00:09, 179.62it/s]Saving vectors of label - 'cat':   4%|3         | 65/1733 [00:00<00:08, 193.45it/s]Saving vectors of label - 'cat':   5%|5         | 90/1733 [00:00<00:07, 207.16it/s]Saving vectors of label - 'cat':   7%|6         | 115/1733 [00:00<00:07, 216.84it/s]Saving vectors of label - 'cat':   8%|8         | 139/1733 [00:00<00:07, 222.87it/s]Saving vectors of label - 'cat':   9%|9         | 162/1733 [00:00<00:07, 223.18it/s]Saving vectors of label - 'cat':  11%|#         | 184/1733 [00:00<00:07, 215.23it/s]Saving vectors of label - 'cat':  12%|#1        | 205/1733 [00:00<00:07, 203.28it/s]Saving vectors of label - 'cat':  13%|#3        | 226/1733 [00:01<00:07, 191.93it/s]Saving vectors of label - 'cat':  14%|#4        | 247/1733 [00:01<00:07, 195.52it/s]Saving vectors of label - 'cat':  15%|#5        | 267/1733 [00:01<00:07, 192.46it/s]Saving vectors of label - 'cat':  17%|#6        | 289/1733 [00:01<00:07, 198.48it/s]Saving vectors of label - 'cat':  18%|#8        | 313/1733 [00:01<00:06, 208.42it/s]Saving vectors of label - 'cat':  19%|#9        | 334/1733 [00:01<00:07, 190.34it/s]Saving vectors of label - 'cat':  20%|##        | 354/1733 [00:01<00:07, 190.51it/s]Saving vectors of label - 'cat':  22%|##1       | 380/1733 [00:01<00:06, 205.31it/s]Saving vectors of label - 'cat':  23%|##3       | 407/1733 [00:01<00:06, 220.80it/s]Saving vectors of label - 'cat':  25%|##4       | 430/1733 [00:02<00:05, 220.47it/s]Saving vectors of label - 'cat':  26%|##6       | 454/1733 [00:02<00:05, 224.24it/s]Saving vectors of label - 'cat':  28%|##7       | 477/1733 [00:02<00:05, 220.30it/s]Saving vectors of label - 'cat':  29%|##8       | 501/1733 [00:02<00:05, 225.40it/s]Saving vectors of label - 'cat':  30%|###       | 525/1733 [00:02<00:05, 227.83it/s]Saving vectors of label - 'cat':  32%|###1      | 548/1733 [00:02<00:05, 224.64it/s]Saving vectors of label - 'cat':  33%|###3      | 572/1733 [00:02<00:05, 227.25it/s]Saving vectors of label - 'cat':  34%|###4      | 595/1733 [00:02<00:05, 224.27it/s]Saving vectors of label - 'cat':  36%|###5      | 618/1733 [00:02<00:04, 224.14it/s]Saving vectors of label - 'cat':  37%|###7      | 642/1733 [00:02<00:04, 227.57it/s]Saving vectors of label - 'cat':  38%|###8      | 665/1733 [00:03<00:04, 218.09it/s]Saving vectors of label - 'cat':  40%|###9      | 689/1733 [00:03<00:04, 222.55it/s]Saving vectors of label - 'cat':  41%|####1     | 713/1733 [00:03<00:04, 225.11it/s]Saving vectors of label - 'cat':  43%|####2     | 737/1733 [00:03<00:04, 228.25it/s]Saving vectors of label - 'cat':  44%|####3     | 762/1733 [00:03<00:04, 231.31it/s]Saving vectors of label - 'cat':  45%|####5     | 786/1733 [00:03<00:04, 229.35it/s]Saving vectors of label - 'cat':  47%|####6     | 809/1733 [00:03<00:04, 215.53it/s]Saving vectors of label - 'cat':  48%|####7     | 831/1733 [00:03<00:04, 190.11it/s]Saving vectors of label - 'cat':  49%|####9     | 851/1733 [00:03<00:04, 191.45it/s]Saving vectors of label - 'cat':  51%|#####     | 877/1733 [00:04<00:04, 207.05it/s]Saving vectors of label - 'cat':  52%|#####1    | 899/1733 [00:04<00:04, 204.48it/s]Saving vectors of label - 'cat':  53%|#####3    | 920/1733 [00:04<00:04, 180.30it/s]Saving vectors of label - 'cat':  54%|#####4    | 940/1733 [00:04<00:04, 184.90it/s]Saving vectors of label - 'cat':  56%|#####5    | 966/1733 [00:04<00:03, 201.16it/s]Saving vectors of label - 'cat':  57%|#####7    | 993/1733 [00:04<00:03, 215.88it/s]Saving vectors of label - 'cat':  59%|#####8    | 1017/1733 [00:04<00:03, 220.29it/s]Saving vectors of label - 'cat':  60%|######    | 1040/1733 [00:04<00:03, 205.96it/s]Saving vectors of label - 'cat':  61%|######1   | 1063/1733 [00:04<00:03, 211.01it/s]Saving vectors of label - 'cat':  63%|######2   | 1090/1733 [00:05<00:02, 224.30it/s]Saving vectors of label - 'cat':  64%|######4   | 1114/1733 [00:05<00:02, 225.11it/s]Saving vectors of label - 'cat':  66%|######5   | 1137/1733 [00:05<00:02, 220.24it/s]Saving vectors of label - 'cat':  67%|######7   | 1163/1733 [00:05<00:02, 229.77it/s]Saving vectors of label - 'cat':  69%|######8   | 1189/1733 [00:05<00:02, 237.60it/s]Saving vectors of label - 'cat':  70%|#######   | 1215/1733 [00:05<00:02, 243.38it/s]Saving vectors of label - 'cat':  72%|#######1  | 1242/1733 [00:05<00:01, 248.24it/s]Saving vectors of label - 'cat':  73%|#######3  | 1268/1733 [00:05<00:01, 246.15it/s]Saving vectors of label - 'cat':  75%|#######4  | 1293/1733 [00:05<00:01, 224.24it/s]Saving vectors of label - 'cat':  76%|#######5  | 1317/1733 [00:06<00:01, 226.98it/s]Saving vectors of label - 'cat':  77%|#######7  | 1343/1733 [00:06<00:01, 233.60it/s]Saving vectors of label - 'cat':  79%|#######8  | 1367/1733 [00:06<00:01, 225.72it/s]Saving vectors of label - 'cat':  80%|########  | 1393/1733 [00:06<00:01, 233.92it/s]Saving vectors of label - 'cat':  82%|########1 | 1420/1733 [00:06<00:01, 242.55it/s]Saving vectors of label - 'cat':  83%|########3 | 1446/1733 [00:06<00:01, 247.02it/s]Saving vectors of label - 'cat':  85%|########4 | 1471/1733 [00:06<00:01, 245.19it/s]Saving vectors of label - 'cat':  86%|########6 | 1496/1733 [00:06<00:00, 239.73it/s]Saving vectors of label - 'cat':  88%|########7 | 1522/1733 [00:06<00:00, 244.25it/s]Saving vectors of label - 'cat':  89%|########9 | 1547/1733 [00:06<00:00, 244.01it/s]Saving vectors of label - 'cat':  91%|######### | 1572/1733 [00:07<00:00, 239.60it/s]Saving vectors of label - 'cat':  92%|#########2| 1597/1733 [00:07<00:00, 242.14it/s]Saving vectors of label - 'cat':  94%|#########3| 1622/1733 [00:07<00:00, 235.66it/s]Saving vectors of label - 'cat':  95%|#########5| 1648/1733 [00:07<00:00, 241.98it/s]Saving vectors of label - 'cat':  97%|#########6| 1673/1733 [00:07<00:00, 243.81it/s]Saving vectors of label - 'cat':  98%|#########8| 1700/1733 [00:07<00:00, 249.89it/s]Saving vectors of label - 'cat': 100%|#########9| 1727/1733 [00:07<00:00, 255.09it/s]Saving vectors of label - 'cat': 100%|##########| 1733/1733 [00:07<00:00, 223.92it/s]
Saving vectors of label - 'happy':   0%|          | 0/1742 [00:00<?, ?it/s]Saving vectors of label - 'happy':   1%|1         | 26/1742 [00:00<00:06, 255.58it/s]Saving vectors of label - 'happy':   3%|3         | 53/1742 [00:00<00:06, 258.44it/s]Saving vectors of label - 'happy':   4%|4         | 73/1742 [00:00<00:07, 234.49it/s]Saving vectors of label - 'happy':   6%|5         | 98/1742 [00:00<00:06, 236.43it/s]Saving vectors of label - 'happy':   7%|7         | 123/1742 [00:00<00:06, 238.47it/s]Saving vectors of label - 'happy':   8%|8         | 145/1742 [00:00<00:06, 231.34it/s]Saving vectors of label - 'happy':  10%|9         | 172/1742 [00:00<00:06, 239.97it/s]Saving vectors of label - 'happy':  11%|#1        | 195/1742 [00:00<00:06, 224.59it/s]Saving vectors of label - 'happy':  12%|#2        | 217/1742 [00:00<00:07, 209.39it/s]Saving vectors of label - 'happy':  14%|#3        | 238/1742 [00:01<00:07, 190.91it/s]Saving vectors of label - 'happy':  15%|#5        | 263/1742 [00:01<00:07, 203.13it/s]Saving vectors of label - 'happy':  16%|#6        | 286/1742 [00:01<00:06, 209.51it/s]Saving vectors of label - 'happy':  18%|#7        | 310/1742 [00:01<00:06, 217.38it/s]Saving vectors of label - 'happy':  19%|#9        | 334/1742 [00:01<00:06, 222.02it/s]Saving vectors of label - 'happy':  20%|##        | 357/1742 [00:01<00:06, 221.93it/s]Saving vectors of label - 'happy':  22%|##1       | 380/1742 [00:01<00:06, 206.38it/s]Saving vectors of label - 'happy':  23%|##3       | 403/1742 [00:01<00:06, 211.35it/s]Saving vectors of label - 'happy':  25%|##4       | 428/1742 [00:01<00:05, 220.61it/s]Saving vectors of label - 'happy':  26%|##5       | 452/1742 [00:02<00:05, 224.37it/s]Saving vectors of label - 'happy':  27%|##7       | 475/1742 [00:02<00:05, 214.82it/s]Saving vectors of label - 'happy':  29%|##8       | 500/1742 [00:02<00:05, 222.66it/s]Saving vectors of label - 'happy':  30%|###       | 526/1742 [00:02<00:05, 231.00it/s]Saving vectors of label - 'happy':  32%|###1      | 550/1742 [00:02<00:05, 218.52it/s]Saving vectors of label - 'happy':  33%|###2      | 573/1742 [00:02<00:05, 197.99it/s]Saving vectors of label - 'happy':  34%|###4      | 594/1742 [00:02<00:06, 181.29it/s]Saving vectors of label - 'happy':  35%|###5      | 613/1742 [00:02<00:06, 174.85it/s]Saving vectors of label - 'happy':  36%|###6      | 632/1742 [00:02<00:06, 171.99it/s]Saving vectors of label - 'happy':  37%|###7      | 650/1742 [00:03<00:06, 159.64it/s]Saving vectors of label - 'happy':  38%|###8      | 667/1742 [00:03<00:07, 150.26it/s]Saving vectors of label - 'happy':  39%|###9      | 685/1742 [00:03<00:06, 157.79it/s]Saving vectors of label - 'happy':  40%|####      | 704/1742 [00:03<00:06, 165.49it/s]Saving vectors of label - 'happy':  42%|####1     | 729/1742 [00:03<00:05, 183.07it/s]Saving vectors of label - 'happy':  43%|####3     | 752/1742 [00:03<00:05, 192.69it/s]Saving vectors of label - 'happy':  44%|####4     | 772/1742 [00:03<00:05, 180.75it/s]Saving vectors of label - 'happy':  45%|####5     | 791/1742 [00:03<00:05, 183.04it/s]Saving vectors of label - 'happy':  46%|####6     | 810/1742 [00:03<00:05, 184.68it/s]Saving vectors of label - 'happy':  48%|####7     | 831/1742 [00:04<00:04, 190.17it/s]Saving vectors of label - 'happy':  49%|####8     | 851/1742 [00:04<00:04, 188.28it/s]Saving vectors of label - 'happy':  50%|#####     | 874/1742 [00:04<00:04, 198.22it/s]Saving vectors of label - 'happy':  51%|#####1    | 895/1742 [00:04<00:04, 180.97it/s]Saving vectors of label - 'happy':  52%|#####2    | 914/1742 [00:04<00:04, 181.62it/s]Saving vectors of label - 'happy':  54%|#####3    | 934/1742 [00:04<00:04, 183.83it/s]Saving vectors of label - 'happy':  55%|#####4    | 955/1742 [00:04<00:04, 190.07it/s]Saving vectors of label - 'happy':  56%|#####5    | 975/1742 [00:04<00:04, 182.54it/s]Saving vectors of label - 'happy':  57%|#####7    | 995/1742 [00:04<00:04, 185.51it/s]Saving vectors of label - 'happy':  58%|#####8    | 1015/1742 [00:05<00:03, 188.17it/s]Saving vectors of label - 'happy':  59%|#####9    | 1034/1742 [00:05<00:03, 187.20it/s]Saving vectors of label - 'happy':  61%|######    | 1057/1742 [00:05<00:03, 196.89it/s]Saving vectors of label - 'happy':  62%|######1   | 1080/1742 [00:05<00:03, 204.29it/s]Saving vectors of label - 'happy':  63%|######3   | 1102/1742 [00:05<00:03, 208.33it/s]Saving vectors of label - 'happy':  65%|######4   | 1126/1742 [00:05<00:02, 215.90it/s]Saving vectors of label - 'happy':  66%|######5   | 1148/1742 [00:05<00:03, 192.79it/s]Saving vectors of label - 'happy':  67%|######7   | 1169/1742 [00:05<00:02, 197.24it/s]Saving vectors of label - 'happy':  68%|######8   | 1190/1742 [00:05<00:03, 179.01it/s]Saving vectors of label - 'happy':  69%|######9   | 1209/1742 [00:06<00:03, 173.35it/s]Saving vectors of label - 'happy':  70%|#######   | 1227/1742 [00:06<00:03, 160.46it/s]Saving vectors of label - 'happy':  71%|#######1  | 1244/1742 [00:06<00:03, 162.40it/s]Saving vectors of label - 'happy':  72%|#######2  | 1262/1742 [00:06<00:02, 166.97it/s]Saving vectors of label - 'happy':  73%|#######3  | 1279/1742 [00:06<00:02, 157.74it/s]Saving vectors of label - 'happy':  74%|#######4  | 1296/1742 [00:06<00:03, 145.63it/s]Saving vectors of label - 'happy':  75%|#######5  | 1311/1742 [00:06<00:03, 136.98it/s]Saving vectors of label - 'happy':  76%|#######6  | 1326/1742 [00:06<00:03, 136.91it/s]Saving vectors of label - 'happy':  77%|#######6  | 1340/1742 [00:07<00:03, 128.12it/s]Saving vectors of label - 'happy':  78%|#######7  | 1354/1742 [00:07<00:03, 128.66it/s]Saving vectors of label - 'happy':  79%|#######8  | 1371/1742 [00:07<00:02, 137.53it/s]Saving vectors of label - 'happy':  80%|#######9  | 1387/1742 [00:07<00:02, 142.31it/s]Saving vectors of label - 'happy':  81%|########  | 1410/1742 [00:07<00:02, 159.11it/s]Saving vectors of label - 'happy':  82%|########2 | 1436/1742 [00:07<00:01, 179.44it/s]Saving vectors of label - 'happy':  84%|########3 | 1456/1742 [00:07<00:01, 166.79it/s]Saving vectors of label - 'happy':  85%|########4 | 1474/1742 [00:07<00:01, 169.71it/s]Saving vectors of label - 'happy':  86%|########5 | 1493/1742 [00:07<00:01, 174.50it/s]Saving vectors of label - 'happy':  87%|########7 | 1516/1742 [00:08<00:01, 185.51it/s]Saving vectors of label - 'happy':  88%|########8 | 1536/1742 [00:08<00:01, 175.34it/s]Saving vectors of label - 'happy':  89%|########9 | 1555/1742 [00:08<00:01, 156.21it/s]Saving vectors of label - 'happy':  90%|######### | 1572/1742 [00:08<00:01, 140.43it/s]Saving vectors of label - 'happy':  91%|#########1| 1587/1742 [00:08<00:01, 125.05it/s]Saving vectors of label - 'happy':  92%|#########1| 1601/1742 [00:08<00:01, 111.42it/s]Saving vectors of label - 'happy':  93%|#########2| 1614/1742 [00:08<00:01, 110.01it/s]Saving vectors of label - 'happy':  93%|#########3| 1627/1742 [00:08<00:01, 113.90it/s]Saving vectors of label - 'happy':  94%|#########4| 1641/1742 [00:09<00:00, 117.99it/s]Saving vectors of label - 'happy':  95%|#########4| 1654/1742 [00:09<00:00, 110.06it/s]Saving vectors of label - 'happy':  96%|#########5| 1666/1742 [00:09<00:00, 111.69it/s]Saving vectors of label - 'happy':  96%|#########6| 1679/1742 [00:09<00:00, 115.77it/s]Saving vectors of label - 'happy':  97%|#########7| 1691/1742 [00:09<00:00, 115.75it/s]Saving vectors of label - 'happy':  98%|#########7| 1705/1742 [00:09<00:00, 119.98it/s]Saving vectors of label - 'happy':  99%|#########8| 1719/1742 [00:09<00:00, 122.50it/s]Saving vectors of label - 'happy':  99%|#########9| 1732/1742 [00:09<00:00, 121.61it/s]Saving vectors of label - 'happy': 100%|##########| 1742/1742 [00:09<00:00, 175.32it/s]
0.0
WARNING: Logging before flag parsing goes to stderr.
W0724 16:47:45.394949 15608 deprecation_wrapper.py:119] From C:\Users\HamzaEhsan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0724 16:47:45.429836 15608 deprecation_wrapper.py:119] From C:\Users\HamzaEhsan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0724 16:47:45.454771 15608 deprecation_wrapper.py:119] From C:\Users\HamzaEhsan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

W0724 16:47:45.469729 15608 deprecation_wrapper.py:119] From C:\Users\HamzaEhsan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

W0724 16:47:45.484688 15608 deprecation_wrapper.py:119] From C:\Users\HamzaEhsan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.

wandb: ERROR wandb.init hasn't been called, can't configure run
W0724 16:47:45.578476 15608 deprecation.py:323] From C:\Users\HamzaEhsan\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\ops\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0724 16:47:45.611382 15608 deprecation_wrapper.py:119] From C:\Users\HamzaEhsan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

Train on 3112 samples, validate on 2076 samples
Epoch 1/50
2019-07-24 16:47:45.667195: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2

  32/3112 [..............................] - ETA: 27s - loss: 11.1792 - acc: 0.2812
1280/3112 [===========>..................] - ETA: 0s - loss: 10.2938 - acc: 0.3297 
2912/3112 [===========================>..] - ETA: 0s - loss: 9.7805 - acc: 0.3592 
3112/3112 [==============================] - 0s 142us/step - loss: 9.7412 - acc: 0.3625 - val_loss: 9.4933 - val_acc: 0.3786
Epoch 2/50

  32/3112 [..............................] - ETA: 0s - loss: 8.9091 - acc: 0.3750
1408/3112 [============>.................] - ETA: 0s - loss: 8.7337 - acc: 0.4311
2944/3112 [===========================>..] - ETA: 0s - loss: 8.4260 - acc: 0.4450
3112/3112 [==============================] - 0s 47us/step - loss: 8.3400 - acc: 0.4508 - val_loss: 7.7666 - val_acc: 0.4769
Epoch 3/50

  32/3112 [..............................] - ETA: 0s - loss: 5.5877 - acc: 0.5938
1792/3112 [================>.............] - ETA: 0s - loss: 6.4593 - acc: 0.5463
3112/3112 [==============================] - 0s 42us/step - loss: 6.1428 - acc: 0.5665 - val_loss: 5.1419 - val_acc: 0.6296
Epoch 4/50

  32/3112 [..............................] - ETA: 0s - loss: 3.8234 - acc: 0.6562
1408/3112 [============>.................] - ETA: 0s - loss: 4.7510 - acc: 0.6562
3040/3112 [============================>.] - ETA: 0s - loss: 4.6994 - acc: 0.6576
3112/3112 [==============================] - 0s 45us/step - loss: 4.6860 - acc: 0.6581 - val_loss: 4.2099 - val_acc: 0.6787
Epoch 5/50

  32/3112 [..............................] - ETA: 0s - loss: 5.9670 - acc: 0.5938
1216/3112 [==========>...................] - ETA: 0s - loss: 4.3092 - acc: 0.6842
2592/3112 [=======================>......] - ETA: 0s - loss: 4.1494 - acc: 0.6987
3112/3112 [==============================] - 0s 52us/step - loss: 4.1092 - acc: 0.6996 - val_loss: 3.5312 - val_acc: 0.7355
Epoch 6/50

  32/3112 [..............................] - ETA: 0s - loss: 5.6352 - acc: 0.6250
1408/3112 [============>.................] - ETA: 0s - loss: 3.5350 - acc: 0.7244
3008/3112 [===========================>..] - ETA: 0s - loss: 3.4833 - acc: 0.7320
3112/3112 [==============================] - 0s 47us/step - loss: 3.4814 - acc: 0.7323 - val_loss: 3.4820 - val_acc: 0.7355
Epoch 7/50

  32/3112 [..............................] - ETA: 0s - loss: 2.4856 - acc: 0.7500
1504/3112 [=============>................] - ETA: 0s - loss: 3.1184 - acc: 0.7586
3008/3112 [===========================>..] - ETA: 0s - loss: 2.9985 - acc: 0.7656
3112/3112 [==============================] - 0s 46us/step - loss: 3.0298 - acc: 0.7635 - val_loss: 3.1112 - val_acc: 0.7558
Epoch 8/50

  32/3112 [..............................] - ETA: 0s - loss: 3.2537 - acc: 0.7188
1344/3112 [===========>..................] - ETA: 0s - loss: 2.8094 - acc: 0.7835
2656/3112 [========================>.....] - ETA: 0s - loss: 2.7306 - acc: 0.7846
3112/3112 [==============================] - 0s 52us/step - loss: 2.7638 - acc: 0.7818 - val_loss: 2.9486 - val_acc: 0.7717
Epoch 9/50

  32/3112 [..............................] - ETA: 0s - loss: 2.4673 - acc: 0.7812
1568/3112 [==============>...............] - ETA: 0s - loss: 2.5985 - acc: 0.7908
3040/3112 [============================>.] - ETA: 0s - loss: 2.6257 - acc: 0.7924
3112/3112 [==============================] - 0s 49us/step - loss: 2.6369 - acc: 0.7921 - val_loss: 2.7633 - val_acc: 0.7775
Epoch 10/50

  32/3112 [..............................] - ETA: 0s - loss: 2.0061 - acc: 0.8125
1568/3112 [==============>...............] - ETA: 0s - loss: 2.4464 - acc: 0.7997
3072/3112 [============================>.] - ETA: 0s - loss: 2.5730 - acc: 0.7982
3112/3112 [==============================] - 0s 46us/step - loss: 2.5593 - acc: 0.7988 - val_loss: 2.7472 - val_acc: 0.7837
Epoch 11/50

  32/3112 [..............................] - ETA: 0s - loss: 0.8998 - acc: 0.8750
1408/3112 [============>.................] - ETA: 0s - loss: 2.6511 - acc: 0.7898
2784/3112 [=========================>....] - ETA: 0s - loss: 2.4625 - acc: 0.8060
3112/3112 [==============================] - 0s 51us/step - loss: 2.4937 - acc: 0.8049 - val_loss: 2.7259 - val_acc: 0.7885
Epoch 12/50

  32/3112 [..............................] - ETA: 0s - loss: 2.9542 - acc: 0.7812
1376/3112 [============>.................] - ETA: 0s - loss: 2.3725 - acc: 0.8103
2720/3112 [=========================>....] - ETA: 0s - loss: 2.4086 - acc: 0.8092
3112/3112 [==============================] - 0s 50us/step - loss: 2.4133 - acc: 0.8085 - val_loss: 2.6153 - val_acc: 0.7895
Epoch 13/50

  32/3112 [..............................] - ETA: 0s - loss: 1.0261 - acc: 0.9375
1472/3112 [=============>................] - ETA: 0s - loss: 2.3212 - acc: 0.8159
2976/3112 [===========================>..] - ETA: 0s - loss: 2.2953 - acc: 0.8196
3112/3112 [==============================] - 0s 45us/step - loss: 2.2836 - acc: 0.8204 - val_loss: 2.5781 - val_acc: 0.7929
Epoch 14/50

  32/3112 [..............................] - ETA: 0s - loss: 3.5973 - acc: 0.7500
1536/3112 [=============>................] - ETA: 0s - loss: 2.2761 - acc: 0.8151
2976/3112 [===========================>..] - ETA: 0s - loss: 2.2000 - acc: 0.8199
3112/3112 [==============================] - 0s 48us/step - loss: 2.2186 - acc: 0.8188 - val_loss: 2.5165 - val_acc: 0.7972
Epoch 15/50

  32/3112 [..............................] - ETA: 0s - loss: 1.0074 - acc: 0.9375
1568/3112 [==============>...............] - ETA: 0s - loss: 1.9779 - acc: 0.8438
3112/3112 [==============================] - 0s 43us/step - loss: 2.1390 - acc: 0.8287 - val_loss: 2.4289 - val_acc: 0.8068
Epoch 16/50

  32/3112 [..............................] - ETA: 0s - loss: 1.0074 - acc: 0.9375
1568/3112 [==============>...............] - ETA: 0s - loss: 1.9418 - acc: 0.8444
3040/3112 [============================>.] - ETA: 0s - loss: 2.0319 - acc: 0.8365
3112/3112 [==============================] - 0s 45us/step - loss: 2.0266 - acc: 0.8371 - val_loss: 2.7037 - val_acc: 0.7813
Epoch 17/50

  32/3112 [..............................] - ETA: 0s - loss: 1.7777 - acc: 0.8125
1600/3112 [==============>...............] - ETA: 0s - loss: 2.0458 - acc: 0.8306
3072/3112 [============================>.] - ETA: 0s - loss: 2.0559 - acc: 0.8317
3112/3112 [==============================] - 0s 45us/step - loss: 2.0577 - acc: 0.8316 - val_loss: 2.4846 - val_acc: 0.7977
Epoch 18/50

  32/3112 [..............................] - ETA: 0s - loss: 1.0946 - acc: 0.9062
1568/3112 [==============>...............] - ETA: 0s - loss: 2.1751 - acc: 0.8253
3112/3112 [==============================] - 0s 43us/step - loss: 1.9554 - acc: 0.8419 - val_loss: 2.4299 - val_acc: 0.8006
Epoch 19/50

  32/3112 [..............................] - ETA: 0s - loss: 1.0386 - acc: 0.9062
1440/3112 [============>.................] - ETA: 0s - loss: 1.8342 - acc: 0.8451
2944/3112 [===========================>..] - ETA: 0s - loss: 1.9140 - acc: 0.8404
3112/3112 [==============================] - 0s 47us/step - loss: 1.9286 - acc: 0.8400 - val_loss: 2.4101 - val_acc: 0.8030
Epoch 20/50

  32/3112 [..............................] - ETA: 0s - loss: 1.0205 - acc: 0.9375
1760/3112 [===============>..............] - ETA: 0s - loss: 1.9072 - acc: 0.8449
3112/3112 [==============================] - 0s 40us/step - loss: 1.8953 - acc: 0.8419 - val_loss: 2.4122 - val_acc: 0.8054
Epoch 21/50

  32/3112 [..............................] - ETA: 0s - loss: 1.5112 - acc: 0.9062
1760/3112 [===============>..............] - ETA: 0s - loss: 2.0830 - acc: 0.8256
3112/3112 [==============================] - 0s 40us/step - loss: 1.9753 - acc: 0.8335 - val_loss: 2.4001 - val_acc: 0.8035
Epoch 22/50

  32/3112 [..............................] - ETA: 0s - loss: 1.5489 - acc: 0.8750
1696/3112 [===============>..............] - ETA: 0s - loss: 1.9527 - acc: 0.8390
3112/3112 [==============================] - 0s 42us/step - loss: 1.8967 - acc: 0.8435 - val_loss: 2.3307 - val_acc: 0.8035
Epoch 23/50

  32/3112 [..............................] - ETA: 0s - loss: 3.7999 - acc: 0.6875
1376/3112 [============>.................] - ETA: 0s - loss: 1.9075 - acc: 0.8452
2624/3112 [========================>.....] - ETA: 0s - loss: 1.8131 - acc: 0.8483
3112/3112 [==============================] - 0s 52us/step - loss: 1.8419 - acc: 0.8470 - val_loss: 2.3847 - val_acc: 0.8011
Epoch 24/50

  32/3112 [..............................] - ETA: 0s - loss: 1.6608 - acc: 0.8438
1472/3112 [=============>................] - ETA: 0s - loss: 1.8267 - acc: 0.8465
2912/3112 [===========================>..] - ETA: 0s - loss: 1.8326 - acc: 0.8403
3112/3112 [==============================] - 0s 47us/step - loss: 1.8101 - acc: 0.8422 - val_loss: 2.4154 - val_acc: 0.8025
Epoch 25/50

  32/3112 [..............................] - ETA: 0s - loss: 2.0904 - acc: 0.8438
1344/3112 [===========>..................] - ETA: 0s - loss: 1.7828 - acc: 0.8452
2720/3112 [=========================>....] - ETA: 0s - loss: 1.8559 - acc: 0.8445
3112/3112 [==============================] - 0s 49us/step - loss: 1.8980 - acc: 0.8390 - val_loss: 2.4114 - val_acc: 0.8006
Epoch 26/50

  32/3112 [..............................] - ETA: 0s - loss: 2.4577 - acc: 0.7812
1632/3112 [==============>...............] - ETA: 0s - loss: 1.7887 - acc: 0.8499
3112/3112 [==============================] - 0s 42us/step - loss: 1.7875 - acc: 0.8519 - val_loss: 2.3552 - val_acc: 0.8121
Epoch 27/50

  32/3112 [..............................] - ETA: 0s - loss: 0.5811 - acc: 0.9375
1376/3112 [============>.................] - ETA: 0s - loss: 1.8845 - acc: 0.8430
2752/3112 [=========================>....] - ETA: 0s - loss: 1.8177 - acc: 0.8452
3112/3112 [==============================] - 0s 52us/step - loss: 1.7730 - acc: 0.8483 - val_loss: 2.3731 - val_acc: 0.8001
Epoch 28/50

  32/3112 [..............................] - ETA: 0s - loss: 2.0766 - acc: 0.8125
1376/3112 [============>.................] - ETA: 0s - loss: 1.6507 - acc: 0.8597
2944/3112 [===========================>..] - ETA: 0s - loss: 1.7496 - acc: 0.8553
3112/3112 [==============================] - 0s 47us/step - loss: 1.7534 - acc: 0.8548 - val_loss: 2.3623 - val_acc: 0.8049
Epoch 29/50

  32/3112 [..............................] - ETA: 0s - loss: 1.7248 - acc: 0.8438
1440/3112 [============>.................] - ETA: 0s - loss: 1.6900 - acc: 0.8528
2976/3112 [===========================>..] - ETA: 0s - loss: 1.6646 - acc: 0.8575
3112/3112 [==============================] - 0s 48us/step - loss: 1.6879 - acc: 0.8570 - val_loss: 2.4119 - val_acc: 0.8011
Epoch 30/50

  32/3112 [..............................] - ETA: 0s - loss: 0.6383 - acc: 0.9062
1728/3112 [===============>..............] - ETA: 0s - loss: 1.7720 - acc: 0.8449
3112/3112 [==============================] - 0s 42us/step - loss: 1.8910 - acc: 0.8380 - val_loss: 2.5087 - val_acc: 0.7943
Epoch 31/50

  32/3112 [..............................] - ETA: 0s - loss: 2.2623 - acc: 0.8438
1440/3112 [============>.................] - ETA: 0s - loss: 1.8874 - acc: 0.8431
2976/3112 [===========================>..] - ETA: 0s - loss: 1.7254 - acc: 0.8508
3112/3112 [==============================] - 0s 46us/step - loss: 1.7324 - acc: 0.8499 - val_loss: 2.3919 - val_acc: 0.7962
Epoch 32/50

  32/3112 [..............................] - ETA: 0s - loss: 0.9433 - acc: 0.9062
1504/3112 [=============>................] - ETA: 0s - loss: 1.6213 - acc: 0.8597
3112/3112 [==============================] - 0s 43us/step - loss: 1.6834 - acc: 0.8544 - val_loss: 2.4623 - val_acc: 0.7900
Epoch 33/50

  32/3112 [..............................] - ETA: 0s - loss: 2.4867 - acc: 0.8438
1696/3112 [===============>..............] - ETA: 0s - loss: 1.6371 - acc: 0.8579
3112/3112 [==============================] - 0s 41us/step - loss: 1.6865 - acc: 0.8535 - val_loss: 2.3466 - val_acc: 0.8011
Epoch 34/50

  32/3112 [..............................] - ETA: 0s - loss: 1.7552 - acc: 0.8750
1408/3112 [============>.................] - ETA: 0s - loss: 1.5491 - acc: 0.8686
2784/3112 [=========================>....] - ETA: 0s - loss: 1.5966 - acc: 0.8639
3112/3112 [==============================] - 0s 49us/step - loss: 1.6612 - acc: 0.8586 - val_loss: 2.3568 - val_acc: 0.8015
Epoch 35/50

  32/3112 [..............................] - ETA: 0s - loss: 1.5442 - acc: 0.8125
1376/3112 [============>.................] - ETA: 0s - loss: 1.6814 - acc: 0.8568
2656/3112 [========================>.....] - ETA: 0s - loss: 1.6775 - acc: 0.8550
3112/3112 [==============================] - 0s 54us/step - loss: 1.6853 - acc: 0.8528 - val_loss: 2.4025 - val_acc: 0.7958
Epoch 36/50

  32/3112 [..............................] - ETA: 0s - loss: 1.0001 - acc: 0.8438
1344/3112 [===========>..................] - ETA: 0s - loss: 1.8619 - acc: 0.8415
2752/3112 [=========================>....] - ETA: 0s - loss: 1.7873 - acc: 0.8441
3112/3112 [==============================] - 0s 50us/step - loss: 1.7330 - acc: 0.8487 - val_loss: 2.3997 - val_acc: 0.8020
Epoch 37/50

  32/3112 [..............................] - ETA: 0s - loss: 1.9396 - acc: 0.8125
1504/3112 [=============>................] - ETA: 0s - loss: 1.5594 - acc: 0.8657
2848/3112 [==========================>...] - ETA: 0s - loss: 1.6730 - acc: 0.8550
3112/3112 [==============================] - 0s 49us/step - loss: 1.6430 - acc: 0.8564 - val_loss: 2.3705 - val_acc: 0.7996
Epoch 38/50

  32/3112 [..............................] - ETA: 0s - loss: 0.6386 - acc: 0.9375
1408/3112 [============>.................] - ETA: 0s - loss: 1.5840 - acc: 0.8679
2720/3112 [=========================>....] - ETA: 0s - loss: 1.5907 - acc: 0.8651
3112/3112 [==============================] - 0s 51us/step - loss: 1.6004 - acc: 0.8641 - val_loss: 2.3489 - val_acc: 0.7977
Epoch 39/50

  32/3112 [..............................] - ETA: 0s - loss: 1.5777 - acc: 0.8438
1280/3112 [===========>..................] - ETA: 0s - loss: 1.5518 - acc: 0.8609
2656/3112 [========================>.....] - ETA: 0s - loss: 1.6408 - acc: 0.8535
3112/3112 [==============================] - 0s 50us/step - loss: 1.6208 - acc: 0.8538 - val_loss: 2.3430 - val_acc: 0.7982
Epoch 40/50

  32/3112 [..............................] - ETA: 0s - loss: 2.8221 - acc: 0.8125
1344/3112 [===========>..................] - ETA: 0s - loss: 1.8319 - acc: 0.8452
2912/3112 [===========================>..] - ETA: 0s - loss: 1.6450 - acc: 0.8585
3112/3112 [==============================] - 0s 47us/step - loss: 1.5957 - acc: 0.8628 - val_loss: 2.3491 - val_acc: 0.7982
Epoch 41/50

  32/3112 [..............................] - ETA: 0s - loss: 0.6281 - acc: 0.9062
1408/3112 [============>.................] - ETA: 0s - loss: 1.5724 - acc: 0.8658
2848/3112 [==========================>...] - ETA: 0s - loss: 1.5665 - acc: 0.8624
3112/3112 [==============================] - 0s 47us/step - loss: 1.5535 - acc: 0.8647 - val_loss: 2.3664 - val_acc: 0.7977
Epoch 42/50

  32/3112 [..............................] - ETA: 0s - loss: 1.5321 - acc: 0.9062
1632/3112 [==============>...............] - ETA: 0s - loss: 1.5251 - acc: 0.8627
3112/3112 [==============================] - 0s 42us/step - loss: 1.5622 - acc: 0.8599 - val_loss: 2.3347 - val_acc: 0.8039
Epoch 43/50

  32/3112 [..............................] - ETA: 0s - loss: 1.0074 - acc: 0.9375
1344/3112 [===========>..................] - ETA: 0s - loss: 1.3762 - acc: 0.8780
2688/3112 [========================>.....] - ETA: 0s - loss: 1.4705 - acc: 0.8717
3112/3112 [==============================] - 0s 51us/step - loss: 1.5118 - acc: 0.8663 - val_loss: 2.3569 - val_acc: 0.7991
Epoch 44/50

  32/3112 [..............................] - ETA: 0s - loss: 0.5037 - acc: 0.9688
1568/3112 [==============>...............] - ETA: 0s - loss: 1.6506 - acc: 0.8457
3112/3112 [==============================] - 0s 43us/step - loss: 1.6130 - acc: 0.8551 - val_loss: 2.3344 - val_acc: 0.7967
Epoch 45/50

  32/3112 [..............................] - ETA: 0s - loss: 0.0261 - acc: 0.9688
1536/3112 [=============>................] - ETA: 0s - loss: 1.4935 - acc: 0.8691
3104/3112 [============================>.] - ETA: 0s - loss: 1.5235 - acc: 0.8628
3112/3112 [==============================] - 0s 45us/step - loss: 1.5196 - acc: 0.8631 - val_loss: 2.3649 - val_acc: 0.7962
Epoch 46/50

  32/3112 [..............................] - ETA: 0s - loss: 1.0179 - acc: 0.9375
1568/3112 [==============>...............] - ETA: 0s - loss: 1.5883 - acc: 0.8635
2880/3112 [==========================>...] - ETA: 0s - loss: 1.5557 - acc: 0.8656
3112/3112 [==============================] - 0s 48us/step - loss: 1.5151 - acc: 0.8692 - val_loss: 2.4075 - val_acc: 0.7982
Epoch 47/50

  32/3112 [..............................] - ETA: 0s - loss: 1.0227 - acc: 0.9375
1408/3112 [============>.................] - ETA: 0s - loss: 1.4444 - acc: 0.8786
2720/3112 [=========================>....] - ETA: 0s - loss: 1.5687 - acc: 0.8632
3112/3112 [==============================] - 0s 50us/step - loss: 1.5440 - acc: 0.8647 - val_loss: 2.3440 - val_acc: 0.7982
Epoch 48/50

  32/3112 [..............................] - ETA: 0s - loss: 1.5136 - acc: 0.9062
1664/3112 [===============>..............] - ETA: 0s - loss: 1.5120 - acc: 0.8642
3112/3112 [==============================] - 0s 42us/step - loss: 1.5619 - acc: 0.8599 - val_loss: 2.3637 - val_acc: 0.7991
Epoch 49/50

  32/3112 [..............................] - ETA: 0s - loss: 1.9363 - acc: 0.8750
1408/3112 [============>.................] - ETA: 0s - loss: 1.4424 - acc: 0.8750
2784/3112 [=========================>....] - ETA: 0s - loss: 1.4776 - acc: 0.8714
3112/3112 [==============================] - 0s 49us/step - loss: 1.5304 - acc: 0.8679 - val_loss: 2.4113 - val_acc: 0.7991
Epoch 50/50

  32/3112 [..............................] - ETA: 0s - loss: 1.8783 - acc: 0.8438
1440/3112 [============>.................] - ETA: 0s - loss: 1.3990 - acc: 0.8701
2720/3112 [=========================>....] - ETA: 0s - loss: 1.4544 - acc: 0.8662
3112/3112 [==============================] - 0s 51us/step - loss: 1.5227 - acc: 0.8609 - val_loss: 2.5240 - val_acc: 0.7909
Traceback (most recent call last):
  File "main.py", line 6, in <module>
    buildModel()
  File "C:\Users\HamzaEhsan\Desktop\speechRecognition\audio.py", line 59, in buildModel
    model.add(LSTM(16, input_shape=(config.buckets, config.max_len, channels), activation="sigmoid"))
  File "C:\Users\HamzaEhsan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\engine\sequential.py", line 165, in add
    layer(x)
  File "C:\Users\HamzaEhsan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\layers\recurrent.py", line 532, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File "C:\Users\HamzaEhsan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\engine\base_layer.py", line 414, in __call__
    self.assert_input_compatibility(inputs)
  File "C:\Users\HamzaEhsan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\engine\base_layer.py", line 311, in assert_input_compatibility
    str(K.ndim(x)))
ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=4
